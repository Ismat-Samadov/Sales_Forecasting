{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression: RMSE = 0.8039139882505983, R^2 = 0.9293120576622209\n",
      "Decision Tree: RMSE = 3.2531526078706373e-14, R^2 = 1.0\n",
      "Random Forest: RMSE = 0.0032960178861133876, R^2 = 0.9999988117569347\n",
      "XGBoost: RMSE = 0.0001283576835521317, R^2 = 0.9999999981979373\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Step 1: Load the Dataset\n",
    "df = pd.read_excel('data/Coffee Shop Sales.xlsx')\n",
    "\n",
    "# Step 2: Data Preprocessing\n",
    "# Calculate Total Sales as (transaction_qty * unit_price)\n",
    "df['Total Sales'] = df['transaction_qty'] * df['unit_price']\n",
    "\n",
    "# Drop any unnecessary columns\n",
    "df = df.drop(['transaction_id', 'transaction_date', 'transaction_time', 'product_detail'], axis=1)\n",
    "\n",
    "# Convert categorical variables into dummy/indicator variables\n",
    "df = pd.get_dummies(df, drop_first=True)\n",
    "\n",
    "# Define feature set X and target y\n",
    "X = df.drop(['Total Sales'], axis=1)  # Features\n",
    "y = df['Total Sales']  # Target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Model Training and Evaluation\n",
    "\n",
    "# Dictionary to store RMSE and R-squared for each model\n",
    "model_results = {}\n",
    "\n",
    "# Linear Regression\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "rmse_lr = np.sqrt(mean_squared_error(y_test, y_pred_lr))\n",
    "r2_lr = r2_score(y_test, y_pred_lr)\n",
    "model_results['Linear Regression'] = {'RMSE': rmse_lr, 'R^2': r2_lr}\n",
    "\n",
    "# Decision Tree\n",
    "dt_model = DecisionTreeRegressor()\n",
    "dt_model.fit(X_train, y_train)\n",
    "y_pred_dt = dt_model.predict(X_test)\n",
    "rmse_dt = np.sqrt(mean_squared_error(y_test, y_pred_dt))\n",
    "r2_dt = r2_score(y_test, y_pred_dt)\n",
    "model_results['Decision Tree'] = {'RMSE': rmse_dt, 'R^2': r2_dt}\n",
    "\n",
    "# Random Forest\n",
    "rf_model = RandomForestRegressor()\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "model_results['Random Forest'] = {'RMSE': rmse_rf, 'R^2': r2_rf}\n",
    "\n",
    "# XGBoost\n",
    "xgb_model = XGBRegressor()\n",
    "xgb_model.fit(X_train, y_train)\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "rmse_xgb = np.sqrt(mean_squared_error(y_test, y_pred_xgb))\n",
    "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
    "model_results['XGBoost'] = {'RMSE': rmse_xgb, 'R^2': r2_xgb}\n",
    "\n",
    "# Step 4: Display Model Results\n",
    "for model_name, metrics in model_results.items():\n",
    "    print(f\"{model_name}: RMSE = {metrics['RMSE']}, R^2 = {metrics['R^2']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-07 12:28:39,462] A new study created in memory with name: no-name-56a553fa-d52f-478f-a7bf-473029dcde79\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing Random Forest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-07 12:29:38,358] Trial 0 finished with value: -0.014424981676025166 and parameters: {'n_estimators': 254, 'max_depth': 10, 'min_samples_split': 9, 'min_samples_leaf': 2}. Best is trial 0 with value: -0.014424981676025166.\n",
      "[I 2024-10-07 12:31:32,516] Trial 1 finished with value: -0.03273287443243654 and parameters: {'n_estimators': 500, 'max_depth': 19, 'min_samples_split': 10, 'min_samples_leaf': 3}. Best is trial 0 with value: -0.014424981676025166.\n",
      "[I 2024-10-07 12:32:38,114] Trial 2 finished with value: -0.013531179723020698 and parameters: {'n_estimators': 286, 'max_depth': 12, 'min_samples_split': 9, 'min_samples_leaf': 2}. Best is trial 2 with value: -0.013531179723020698.\n",
      "[I 2024-10-07 12:34:29,192] Trial 3 finished with value: -0.004655442846965939 and parameters: {'n_estimators': 486, 'max_depth': 15, 'min_samples_split': 5, 'min_samples_leaf': 1}. Best is trial 3 with value: -0.004655442846965939.\n",
      "[I 2024-10-07 12:35:36,614] Trial 4 finished with value: -0.12891494873622986 and parameters: {'n_estimators': 293, 'max_depth': 11, 'min_samples_split': 8, 'min_samples_leaf': 4}. Best is trial 3 with value: -0.004655442846965939.\n",
      "[I 2024-10-07 12:37:23,435] Trial 5 finished with value: -0.032688697226197064 and parameters: {'n_estimators': 457, 'max_depth': 14, 'min_samples_split': 3, 'min_samples_leaf': 3}. Best is trial 3 with value: -0.004655442846965939.\n",
      "[I 2024-10-07 12:37:50,300] Trial 6 finished with value: -0.004054764618934685 and parameters: {'n_estimators': 115, 'max_depth': 17, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 6 with value: -0.004054764618934685.\n",
      "[I 2024-10-07 12:39:12,597] Trial 7 finished with value: -0.013052190596183424 and parameters: {'n_estimators': 357, 'max_depth': 17, 'min_samples_split': 8, 'min_samples_leaf': 2}. Best is trial 6 with value: -0.004054764618934685.\n",
      "[I 2024-10-07 12:39:50,229] Trial 8 finished with value: -0.1358439975640547 and parameters: {'n_estimators': 164, 'max_depth': 14, 'min_samples_split': 4, 'min_samples_leaf': 4}. Best is trial 6 with value: -0.004054764618934685.\n",
      "[I 2024-10-07 12:40:41,141] Trial 9 finished with value: -0.486542479022705 and parameters: {'n_estimators': 300, 'max_depth': 5, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 6 with value: -0.004054764618934685.\n",
      "[I 2024-10-07 12:40:54,717] Trial 10 finished with value: -0.004430347398017035 and parameters: {'n_estimators': 59, 'max_depth': 20, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 6 with value: -0.004054764618934685.\n",
      "[I 2024-10-07 12:41:07,163] Trial 11 finished with value: -0.0042311069531504 and parameters: {'n_estimators': 54, 'max_depth': 20, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 6 with value: -0.004054764618934685.\n",
      "[I 2024-10-07 12:41:19,817] Trial 12 finished with value: -0.00430416889587044 and parameters: {'n_estimators': 55, 'max_depth': 18, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 6 with value: -0.004054764618934685.\n",
      "[I 2024-10-07 12:41:52,817] Trial 13 finished with value: -0.004707488998310558 and parameters: {'n_estimators': 143, 'max_depth': 17, 'min_samples_split': 6, 'min_samples_leaf': 1}. Best is trial 6 with value: -0.004054764618934685.\n",
      "[I 2024-10-07 12:42:26,470] Trial 14 finished with value: -0.01743804130746474 and parameters: {'n_estimators': 145, 'max_depth': 20, 'min_samples_split': 4, 'min_samples_leaf': 2}. Best is trial 6 with value: -0.004054764618934685.\n",
      "[I 2024-10-07 12:42:51,164] Trial 15 finished with value: -0.004285463135328753 and parameters: {'n_estimators': 107, 'max_depth': 16, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 6 with value: -0.004054764618934685.\n",
      "[I 2024-10-07 12:43:35,707] Trial 16 finished with value: -0.003921368789160835 and parameters: {'n_estimators': 195, 'max_depth': 9, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 16 with value: -0.003921368789160835.\n",
      "[I 2024-10-07 12:44:22,107] Trial 17 finished with value: -0.039153441138396985 and parameters: {'n_estimators': 210, 'max_depth': 8, 'min_samples_split': 6, 'min_samples_leaf': 3}. Best is trial 16 with value: -0.003921368789160835.\n",
      "[I 2024-10-07 12:45:12,266] Trial 18 finished with value: -0.01459528106173873 and parameters: {'n_estimators': 220, 'max_depth': 9, 'min_samples_split': 4, 'min_samples_leaf': 2}. Best is trial 16 with value: -0.003921368789160835.\n",
      "[I 2024-10-07 12:45:52,582] Trial 19 finished with value: -0.022790692262966962 and parameters: {'n_estimators': 194, 'max_depth': 7, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 16 with value: -0.003921368789160835.\n",
      "[I 2024-10-07 12:47:03,189] Trial 20 finished with value: -0.12460875661476165 and parameters: {'n_estimators': 371, 'max_depth': 6, 'min_samples_split': 5, 'min_samples_leaf': 2}. Best is trial 16 with value: -0.003921368789160835.\n",
      "[I 2024-10-07 12:47:26,781] Trial 21 finished with value: -0.0038822035838252536 and parameters: {'n_estimators': 102, 'max_depth': 18, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 21 with value: -0.0038822035838252536.\n",
      "[I 2024-10-07 12:47:50,599] Trial 22 finished with value: -0.0038878257719244637 and parameters: {'n_estimators': 103, 'max_depth': 12, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 21 with value: -0.0038822035838252536.\n",
      "[I 2024-10-07 12:48:15,146] Trial 23 finished with value: -0.004165637918218039 and parameters: {'n_estimators': 106, 'max_depth': 12, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 21 with value: -0.0038822035838252536.\n",
      "[I 2024-10-07 12:48:54,462] Trial 24 finished with value: -0.003989291378911404 and parameters: {'n_estimators': 172, 'max_depth': 9, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 21 with value: -0.0038822035838252536.\n",
      "[I 2024-10-07 12:49:17,431] Trial 25 finished with value: -0.018378129477026543 and parameters: {'n_estimators': 99, 'max_depth': 14, 'min_samples_split': 3, 'min_samples_leaf': 2}. Best is trial 21 with value: -0.0038822035838252536.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import optuna\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Step 1: Load the Dataset\n",
    "df = pd.read_excel('data/Coffee Shop Sales.xlsx')\n",
    "\n",
    "# Step 2: Data Preprocessing\n",
    "# Calculate Total Sales as (transaction_qty * unit_price)\n",
    "df['Total Sales'] = df['transaction_qty'] * df['unit_price']\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df = df.drop(['transaction_id', 'transaction_date', 'transaction_time', 'product_detail'], axis=1)\n",
    "\n",
    "# Convert categorical variables into dummy/indicator variables\n",
    "df = pd.get_dummies(df, drop_first=True)\n",
    "\n",
    "# Define feature set X and target y\n",
    "X = df.drop(['Total Sales'], axis=1)  # Features\n",
    "y = df['Total Sales']  # Target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Define the objective functions for Random Forest and XGBoost with Optuna\n",
    "\n",
    "def rf_objective(trial):\n",
    "    \"\"\"Objective function for Random Forest optimization with Optuna.\"\"\"\n",
    "    # Define hyperparameter search space\n",
    "    n_estimators = trial.suggest_int('n_estimators', 50, 500)\n",
    "    max_depth = trial.suggest_int('max_depth', 5, 20)\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 2, 10)\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 4)\n",
    "    \n",
    "    rf = RandomForestRegressor(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(rf, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "    return np.mean(cv_scores)\n",
    "\n",
    "def xgb_objective(trial):\n",
    "    \"\"\"Objective function for XGBoost optimization with Optuna.\"\"\"\n",
    "    # Define hyperparameter search space\n",
    "    n_estimators = trial.suggest_int('n_estimators', 50, 500)\n",
    "    max_depth = trial.suggest_int('max_depth', 3, 15)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 0.01, 0.3)\n",
    "    colsample_bytree = trial.suggest_float('colsample_bytree', 0.3, 1.0)\n",
    "    \n",
    "    xgb = XGBRegressor(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        learning_rate=learning_rate,\n",
    "        colsample_bytree=colsample_bytree,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(xgb, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "    return np.mean(cv_scores)\n",
    "\n",
    "# Step 4: Optimize Random Forest\n",
    "print(\"Optimizing Random Forest...\")\n",
    "rf_study = optuna.create_study(direction='maximize')\n",
    "rf_study.optimize(rf_objective, n_trials=50)\n",
    "\n",
    "# Step 5: Optimize XGBoost\n",
    "print(\"Optimizing XGBoost...\")\n",
    "xgb_study = optuna.create_study(direction='maximize')\n",
    "xgb_study.optimize(xgb_objective, n_trials=50)\n",
    "\n",
    "# Step 6: Train the best models using the optimized hyperparameters\n",
    "\n",
    "# Random Forest\n",
    "best_rf_params = rf_study.best_params\n",
    "print(\"Best Random Forest Params: \", best_rf_params)\n",
    "rf_model = RandomForestRegressor(**best_rf_params, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# XGBoost\n",
    "best_xgb_params = xgb_study.best_params\n",
    "print(\"Best XGBoost Params: \", best_xgb_params)\n",
    "xgb_model = XGBRegressor(**best_xgb_params, random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "# Step 7: Evaluate the models with the optimized parameters\n",
    "print(\"\\nModel Performance After Optimization:\\n\")\n",
    "\n",
    "# Random Forest\n",
    "rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "print(f\"Random Forest - RMSE: {rmse_rf}, R²: {r2_rf}\")\n",
    "\n",
    "# XGBoost\n",
    "rmse_xgb = np.sqrt(mean_squared_error(y_test, y_pred_xgb))\n",
    "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
    "print(f\"XGBoost - RMSE: {rmse_xgb}, R²: {r2_xgb}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
